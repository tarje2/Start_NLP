{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN-LSTM 기반의 영어 감정 분석기\n",
    "##### FRIENDS 시트콤 데이터 - 모델 실행 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\server\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\server\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re # 축약문 치환\n",
    "from soynlp.normalizer import * # 반복문자 교정\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence # 토큰화\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "from nltk.corpus import stopwords # StopWord(불용어)\n",
    "stopwords.words('english')\n",
    "\n",
    "new_stopwords = stopwords.words('english')\n",
    "new_stopwords.append(\"phoebe\")\n",
    "new_stopwords.append(\"monica\")\n",
    "new_stopwords.append(\"ross\")\n",
    "new_stopwords.append(\"chandler\")\n",
    "new_stopwords.append(\"joey\")\n",
    "new_stopwords.append(\"rachel\")\n",
    "new_stopwords.append(\"dr\")\n",
    "new_stopwords.append(\"ms\")\n",
    "new_stopwords.append(\"mr\")\n",
    "new_stopwords.append(\"mrs\")\n",
    "\n",
    "## Lemma\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "n = WordNetLemmatizer()\n",
    "\n",
    "## Stem\n",
    "# Stemming_Porter\n",
    "from nltk.stem import PorterStemmer\n",
    "p = PorterStemmer()\n",
    "\n",
    "# Stemming_Lancaster\n",
    "from nltk.stem import LancasterStemmer\n",
    "l=LancasterStemmer()\n",
    "\n",
    "# Stemming_SnowballStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "s = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule-based 축약문 치환\n",
    "def char_change(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    sentence = re.sub(r\"ain't\", \"have not\", sentence)\n",
    "    sentence = re.sub(r\"can't\", \"can not\", sentence)\n",
    "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"y'\", \"you \", sentence)\n",
    "    sentence = re.sub(r\"'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "    sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "    sentence = re.sub(r\"there's\", \"there is\", sentence)\n",
    "    sentence = re.sub(r\"what's\", \"what is\", sentence)\n",
    "    sentence = re.sub(r\"who's\", \"who is\", sentence)\n",
    "    sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "    sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "    sentence = re.sub(r\"let's\", \"let us\", sentence)\n",
    "    sentence = re.sub(r\"c'mon\", \"come on\", sentence)\n",
    "    sentence = re.sub(r\"c'mere\", \"come here\", sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    \n",
    "    #전처리(영어, 공백을 제외한 문자 제거)\n",
    "    new_sentence = re.sub(r\"[^A-Za-z ']\", \"\", new_sentence)\n",
    "    \n",
    "    # 축약문 처리\n",
    "    temp_X2 = char_change(new_sentence)\n",
    "    \n",
    "    # 반복문자 교정\n",
    "    temp_X2 = repeat_normalize(temp_X2, num_repeats=1)\n",
    "    \n",
    "    #토큰화\n",
    "    new_sentence = text_to_word_sequence(new_sentence)  \n",
    "    \n",
    "    #Lemmatization\n",
    "    new_sentence = [n.lemmatize(word, 'v') for word in new_sentence] \n",
    "    \n",
    "    # Stemming_Porter\n",
    "    #new_sentence = [p.stem(word) for word in new_sentence]\n",
    "    \n",
    "    # Stemming_Lancaster\n",
    "    #new_sentence = [l.stem(word) for word in new_sentence]\n",
    "    \n",
    "    # Stemming_SnowballStemmer\n",
    "    #new_sentence = [s.stem(word) for word in new_sentence]\n",
    "    \n",
    "    #Stopword(불용어) 제거\n",
    "    new_sentence = [word for word in new_sentence if not word in new_stopwords]\n",
    "    #new_sentence = [word for word in new_sentence if not word in stopwords.words('english')]\n",
    "    \n",
    "    #정수 인코딩\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence])\n",
    "    \n",
    "    #패딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len)\n",
    "    \n",
    "    score = loaded_model2.predict(pad_new)\n",
    "\n",
    "    #val = np.max(score)\n",
    "    val_index = np.argmax(score)\n",
    "\n",
    "    if(val_index==0):\n",
    "        return \"anger\"\n",
    "    elif(val_index==1):\n",
    "        return \"disgust\"\n",
    "    elif(val_index==2):\n",
    "        return \"fear\"\n",
    "    elif(val_index==3):\n",
    "        return \"joy\"\n",
    "    elif(val_index==4):\n",
    "        return \"neutral\"\n",
    "    elif(val_index==5):\n",
    "        return \"non-neutral\"\n",
    "    elif(val_index==6):\n",
    "        return \"sadness\"\n",
    "    elif(val_index==7):\n",
    "        return \"surprise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-03cf9d4859aa>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data['Predicted'][i] = Predicted\n",
      "C:\\Users\\server\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "loaded_model2 = load_model('friends_best_model.h5')\n",
    "\n",
    "eval_data = pd.read_csv('./[전처리]en_data.csv', engine='python', encoding='utf-8')\n",
    "eval_data['Predicted'] = 0\n",
    "\n",
    "for i in range(len(eval_data)):\n",
    "    Predicted = sentiment_predict(eval_data['utterance'][i])\n",
    "    eval_data['Predicted'][i] = Predicted\n",
    "    \n",
    "eval_data.drop(['i_dialog'], axis='columns', inplace=True)\n",
    "eval_data.drop(['i_utterance'], axis='columns', inplace=True)\n",
    "eval_data.drop(['speaker'], axis='columns', inplace=True)\n",
    "eval_data.drop(['utterance'], axis='columns', inplace=True)\n",
    "\n",
    "eval_data.to_csv(\"result_2019512014_이동환.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
