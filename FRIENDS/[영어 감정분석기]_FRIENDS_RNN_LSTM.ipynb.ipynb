{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN-LSTM 기반의 영어 감정 분석기\n",
    "##### FRIENDS 시트콤 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_Set :  10561\n",
      "Dev_Set :  1178\n",
      "Test_Set :  2764\n"
     ]
    }
   ],
   "source": [
    "## 데이터셋 로드\n",
    "\n",
    "# Training Set\n",
    "train_data2 = pd.read_csv('./friends_train_아스키제거_감정코드.csv')\n",
    "\n",
    "# Dev Set\n",
    "dev_data2 = pd.read_csv('./friends_dev_아스키제거_감정코드.csv')\n",
    "\n",
    "# Test Set\n",
    "test_data2 = pd.read_csv('./friends_test_아스키제거_감정코드.csv')\n",
    "\n",
    "print(\"Training_Set : \", len(train_data2))\n",
    "print(\"Dev_Set : \", len(dev_data2))\n",
    "print(\"Test_Set : \", len(test_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 정제 및 전처리(Train, Dev, Test)\n",
    "##### 1) 중복 제거\n",
    "##### 2) Null값 제거\n",
    "##### 3) 영어, 공백, ' 제외 문자 제거\n",
    "##### 4) 전처리 후 Null 값 제거\n",
    "##### 데이터 1차 저장(train_data_set.csv, dev_data_set.csv, test_data_set.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Data 처리\n",
    "\n",
    "# 1. 중복 확인/제거\n",
    "train_data2['utterance'].nunique(), train_data2['emotion'].nunique()\n",
    "train_data2.drop_duplicates(subset=['utterance'], inplace=True)\n",
    "\n",
    "# 2. Null 값이 존재하는 행 제거\n",
    "train_data2 = train_data2.dropna(how = 'any')\n",
    "\n",
    "# 3. 전처리(영어, 공백, ' 을 제외한 문자 제거)\n",
    "train_data2['utterance'] = train_data2['utterance'].str.replace(\"[^A-Za-z ']\",\"\")\n",
    "train_data2['utterance'] = train_data2['utterance'].str.strip()\n",
    "train_data2['utterance'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# 4. 전처리 후 Null값 처리\n",
    "train_data2 = train_data2.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dev Data 처리\n",
    "\n",
    "# 1. 중복 확인/제거\n",
    "dev_data2['utterance'].nunique(), dev_data2['emotion'].nunique()\n",
    "dev_data2.drop_duplicates(subset=['utterance'], inplace=True)\n",
    "\n",
    "# 2. Null 값이 존재하는 행 제거\n",
    "dev_data2 = dev_data2.dropna(how = 'any')\n",
    "\n",
    "# 3. 전처리(영어, 공백, ' 을 제외한 문자 제거)\n",
    "dev_data2['utterance'] = dev_data2['utterance'].str.replace(\"[^A-Za-z ']\",\"\")\n",
    "dev_data2['utterance'] = dev_data2['utterance'].str.strip()\n",
    "dev_data2['utterance'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# 4. 전처리 후 Null값 처리\n",
    "dev_data2 = dev_data2.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Data 처리\n",
    "\n",
    "# 1. 중복 확인/제거\n",
    "test_data2['utterance'].nunique(), test_data2['emotion'].nunique()\n",
    "test_data2.drop_duplicates(subset=['utterance'], inplace=True)\n",
    "\n",
    "# 2. Null 값이 존재하는 행 제거\n",
    "test_data2 = test_data2.dropna(how = 'any')\n",
    "\n",
    "# 3. 전처리(영어, 공백, ' 을 제외한 문자 제거)\n",
    "test_data2['utterance'] = test_data2['utterance'].str.replace(\"[^A-Za-z ']\",\"\")\n",
    "test_data2['utterance'] = test_data2['utterance'].str.strip()\n",
    "test_data2['utterance'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# 4. 전처리 후 Null값 처리\n",
    "test_data2 = test_data2.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 수(Training_Set) : 9268\n",
      "총 샘플의 수(Dev_Set) : 1088\n",
      "총 샘플의 수(Test_Set) : 2500\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 수(Training_Set) :',len(train_data2))\n",
    "print('총 샘플의 수(Dev_Set) :',len(dev_data2))\n",
    "print('총 샘플의 수(Test_Set) :',len(test_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 전처리 결과 저장\n",
    "#중복, null값, 전처리(영어, 공백 제외), 전처리 후 null값 처리까지 한 데이터 저장\n",
    "\n",
    "# Train Data\n",
    "train_data2.to_csv(\"train_data_set.csv\", mode='w', index=False)\n",
    "\n",
    "# Dev Data\n",
    "dev_data2.to_csv(\"dev_data_set.csv\", mode='w', index=False)\n",
    "\n",
    "# Test Data\n",
    "test_data2.to_csv(\"test_data_set.csv\", mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 전처리 결과를 저장한 데이터 불러오기\n",
    "\n",
    "train_data2 = pd.read_csv('./train_data_set.csv')\n",
    "dev_data2 = pd.read_csv('./dev_data_set.csv')\n",
    "test_data2 = pd.read_csv('./test_data_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\server\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\server\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re # 축약문 치환\n",
    "from soynlp.normalizer import * # 반복문자 교정\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence # 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "from nltk.corpus import stopwords # StopWord(불용어)\n",
    "stopwords.words('english')\n",
    "\n",
    "new_stopwords = stopwords.words('english')\n",
    "new_stopwords.append(\"he's\")\n",
    "new_stopwords.append(\"phoebe\")\n",
    "new_stopwords.append(\"monica\")\n",
    "new_stopwords.append(\"ross\")\n",
    "new_stopwords.append(\"chandler\")\n",
    "new_stopwords.append(\"joey\")\n",
    "new_stopwords.append(\"rachel\")\n",
    "new_stopwords.append(\"dr\")\n",
    "new_stopwords.append(\"ms\")\n",
    "new_stopwords.append(\"mr\")\n",
    "new_stopwords.append(\"mrs\")\n",
    "\n",
    "## Lemma\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "n = WordNetLemmatizer()\n",
    "\n",
    "## Stem\n",
    "# Stemming_Porter\n",
    "from nltk.stem import PorterStemmer\n",
    "p = PorterStemmer()\n",
    "\n",
    "# Stemming_Lancaster\n",
    "from nltk.stem import LancasterStemmer\n",
    "l=LancasterStemmer()\n",
    "\n",
    "# Stemming_SnowballStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "s = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule-based 축약문 치환\n",
    "def char_change(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    sentence = re.sub(r\"ain't\", \"have not\", sentence)\n",
    "    sentence = re.sub(r\"can't\", \"can not\", sentence)\n",
    "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"y'\", \"you \", sentence)\n",
    "    sentence = re.sub(r\"'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "    sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "    sentence = re.sub(r\"there's\", \"there is\", sentence)\n",
    "    sentence = re.sub(r\"what's\", \"what is\", sentence)\n",
    "    sentence = re.sub(r\"who's\", \"who is\", sentence)\n",
    "    sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "    sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "    sentence = re.sub(r\"let's\", \"let us\", sentence)\n",
    "    sentence = re.sub(r\"c'mon\", \"come on\", sentence)\n",
    "    sentence = re.sub(r\"c'mere\", \"come here\", sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train 데이터\n",
    "\n",
    "X_train2 = []\n",
    "\n",
    "for sentence in train_data2['utterance']:\n",
    "    temp_X2 = []\n",
    "    \n",
    "    # 축약문 교정\n",
    "    temp_X2 = char_change(sentence)\n",
    "    \n",
    "    # 반복문자 교정\n",
    "    temp_X2 = repeat_normalize(temp_X2, num_repeats=1)\n",
    "    \n",
    "    # 토큰화\n",
    "    temp_X2 = text_to_word_sequence(temp_X2)\n",
    "    \n",
    "    # Stopwords(불용어) 제거\n",
    "    #temp_X2 = [word for word in temp_X2 if not word in stopwords.words('english')]\n",
    "    temp_X2 = [word for word in temp_X2 if not word in new_stopwords]\n",
    "    \n",
    "    # Lemmatization\n",
    "    temp_X2 = [n.lemmatize(word, 'v') for word in temp_X2]\n",
    "    \n",
    "    # Stemming_Porter\n",
    "    #temp_X2 = [p.stem(word) for word in temp_X2]\n",
    "    \n",
    "    # Stemming_Lancaster\n",
    "    #temp_X2 = [l.stem(word) for word in temp_X2]\n",
    "    \n",
    "    # Stemming_SnowballStemmer\n",
    "    #temp_X2 = [s.stem(word) for word in temp_X2]\n",
    "    \n",
    "    X_train2.append(temp_X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dev 데이터\n",
    "\n",
    "X_dev2 = []\n",
    "\n",
    "for sentence in dev_data2['utterance']:\n",
    "    temp_X2 = []\n",
    "    \n",
    "    # 축약문 교정\n",
    "    temp_X2 = char_change(sentence)\n",
    "    \n",
    "    # 반복문자 교정\n",
    "    temp_X2 = repeat_normalize(temp_X2, num_repeats=1)\n",
    "    \n",
    "    # 토큰화\n",
    "    temp_X2 = text_to_word_sequence(temp_X2)\n",
    "    \n",
    "    # 불용어 제거\n",
    "    #temp_X2 = [word for word in temp_X2 if not word in stopwords.words('english')]\n",
    "    temp_X2 = [word for word in temp_X2 if not word in new_stopwords]\n",
    "    \n",
    "    # Lemmatization\n",
    "    temp_X2 = [n.lemmatize(word, 'v') for word in temp_X2]\n",
    "    \n",
    "    # Stemming_Porter\n",
    "    #temp_X2 = [p.stem(word) for word in temp_X2]\n",
    "    \n",
    "    # Stemming_Lancaster\n",
    "    #temp_X2 = [l.stem(word) for word in temp_X2]\n",
    "    \n",
    "    # Stemming_SnowballStemmer\n",
    "    #temp_X2 = [s.stem(word) for word in temp_X2]\n",
    "    \n",
    "    X_dev2.append(temp_X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test 데이터\n",
    "\n",
    "X_test2 = []\n",
    "\n",
    "for sentence in test_data2['utterance']:\n",
    "    temp_X2 = []\n",
    "    \n",
    "    # 축약문 교정\n",
    "    temp_X2 = char_change(sentence)\n",
    "    \n",
    "    # 반복문자 교정\n",
    "    temp_X2 = repeat_normalize(temp_X2, num_repeats=1)    \n",
    "    \n",
    "    # 토큰화\n",
    "    temp_X2 = text_to_word_sequence(temp_X2)\n",
    "    \n",
    "    # 불용어 제거\n",
    "    #temp_X2 = [word for word in temp_X2 if not word in stopwords.words('english')]\n",
    "    temp_X2 = [word for word in temp_X2 if not word in new_stopwords]\n",
    "    \n",
    "    # Lemmatization\n",
    "    temp_X2 = [n.lemmatize(word, 'v') for word in temp_X2]\n",
    "    \n",
    "    # Stemming_Porter\n",
    "    #temp_X2 = [p.stem(word) for word in temp_X2]\n",
    "    \n",
    "    # Stemming_Lancaster\n",
    "    #temp_X = [l.stem(word) for word in temp_X2]\n",
    "    \n",
    "    # Stemming_SnowballStemmer\n",
    "    #emp_X2 = [s.stem(word) for word in temp_X2]\n",
    "    \n",
    "    X_test2.append(temp_X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['know'], ['mean', 'end', 'era'], ['gonna', 'miss'], ['oh', 'god', 'gonna', 'miss', 'much'], [], ['mean', 'gonna', 'live', 'together', 'anymore'], ['yeah', 'sweetie'], ['really'], [], ['something', 'impulsively', 'decide', 'inin', 'vegas', 'something', 'really', 'want'], ['let', 'finish', 'okay'], ['nono', 'wait'], [], ['go', 'happen', 'gonna', 'move'], ['right', 'hang'], [], ['well', 'pretty', 'much', 'gist', 'well', 'except', 'poem', 'read', 'poem', 'right'], ['goodbye', 'ursula', 'miss', 'ps', 'mom', 'live', 'montauk', 'write'], ['umm', 'shut'], ['saw', 'check', 'game', 'last', 'night'], ['huh'], ['still', 'think', 'boyfriend', 'material'], ['white', 'plain', 'oh', 'sound', 'like', 'magical', 'place'], ['yes', 'know', 'white', 'plain'], ['like', 'know'], ['tell', 'anything'], ['bet', 'great', 'story', 'behind'], ['wow'], ['god', 'oh', 'perfect'], ['yes', 'yes'], ['uh', 'yore', 'like', 'days', 'yore', 'know'], ['well', 'period'], ['yeah'], ['ohh', 'okay', 'give', 'old', 'time', 'price'], ['could', 'find', 'anything', 'joeyheyhey', 'oh', 'hey'], ['eat', 'skin', \"chicken's\", 'grab'], ['right', 'go', 'get', 'back', 'bucket'], ['eh'], ['well', 'think', 'learn', 'something', 'disgust'], ['disgust', 'stalk', 'guy', 'keep', 'underpants'], ['unbelievable', 'mean', 'really', 'keep', 'joeyou', 'underwear', 'would'], ['sadly', 'could', 'entice'], ['guess', 'set', 'video', 'camera', 'try', 'entice'], ['oh', 'honey', 'worry', 'gonna', 'make', 'time'], [\"alice's\", 'mom', 'say', 'leave', 'five', 'hours', 'ago'], ['hi'], ['yeah', 'love', 'okay', 'bye', 'hi'], ['right', 'hey', 'better', 'make', 'look', 'really', 'really', 'good', 'oh', 'another', 'thing', 'video', 'camera', 'nice'], ['hey', 'anyway'], ['oh', 'kinda', 'hop']]\n"
     ]
    }
   ],
   "source": [
    "print(X_dev2[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도순 단어 집합 생성 -> 높은 정수값은 빈도가 낮은 것\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train2)\n",
    "\n",
    "#print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4993\n"
     ]
    }
   ],
   "source": [
    "total_cnt = len(tokenizer.word_index)\n",
    "print(total_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[총 단어 수] 단어 집합(vocabulary)의 크기 : 4993\n",
      "등장 빈도가 1번 이하인 희귀 단어의 수: 2707\n",
      "단어 집합에서 희귀 단어의 비율: 54.21590226316844\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 7.188379627170853\n"
     ]
    }
   ],
   "source": [
    "threshold = 2\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('[총 단어 수] 단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 2288\n"
     ]
    }
   ],
   "source": [
    "# 전체 단어 개수 중 빈도수 2번 이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩 과정에서 이보다 큰 숫자가 부여된 단어들은 OOV로 변환하겠습니다. 다시 말해 정수 1번으로 할당\n",
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train2)\n",
    "\n",
    "X_train2 = tokenizer.texts_to_sequences(X_train2)\n",
    "X_dev2 = tokenizer.texts_to_sequences(X_dev2)\n",
    "X_test2 = tokenizer.texts_to_sequences(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2 = np.array(train_data2['emotion_code'])\n",
    "y_dev2 = np.array(dev_data2['emotion_code'])\n",
    "y_test2 = np.array(test_data2['emotion_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = np.array(X_train2)\n",
    "X_dev2 = np.array(X_dev2)\n",
    "X_test2 = np.array(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 빈 샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train2 = [index for index, sentence in enumerate(X_train2) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(drop_train2)\n",
    "# 빈 샘플들을 제거\n",
    "X_train2 = np.delete(X_train2, drop_train2, axis=0)\n",
    "y_train2 = np.delete(y_train2, drop_train2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대사 최대 길이 : 32\n",
      "대사 평균 길이 : 4.203370911932136\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('대사 최대 길이 :',max(len(l) for l in X_train2))\n",
    "print('대사 평균 길이 :',sum(map(len, X_train2))/len(X_train2))\n",
    "plt.hist([len(s) for s in X_train2], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 샘플 중 길이가 max_len 이하인 샘플의 비율이 몇 %인지 확인하는 함수\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 10 이하인 샘플의 비율: 96.26074338653868\n"
     ]
    }
   ],
   "source": [
    "max_len = 10\n",
    "below_threshold_len(max_len, X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 샘플 길이 통일\n",
    "X_train2 = pad_sequences(X_train2, maxlen=max_len)\n",
    "X_dev2 = pad_sequences(X_dev2, maxlen=max_len)\n",
    "X_test2 = pad_sequences(X_test2, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련/검증/테스트용 레이블의 원-핫 인코딩\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train2 = to_categorical(y_train2)\n",
    "y_dev2 = to_categorical(y_dev2)\n",
    "y_test2 = to_categorical(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. LSTM 모델 설정 및 학습/평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Embedding(vocab_size, 128))\n",
    "model2.add(LSTM(128))\n",
    "model2.add(Dense(8, activation='softmax'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('friends_best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "134/140 [===========================>..] - ETA: 0s - loss: 1.6433 - acc: 0.4374\n",
      "Epoch 00001: val_acc improved from -inf to 0.41085, saving model to friends_best_model.h5\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 1.6378 - acc: 0.4400 - val_loss: 1.7036 - val_acc: 0.4108\n",
      "Epoch 2/10\n",
      "139/140 [============================>.] - ETA: 0s - loss: 1.4858 - acc: 0.4753\n",
      "Epoch 00002: val_acc improved from 0.41085 to 0.43658, saving model to friends_best_model.h5\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 1.4852 - acc: 0.4754 - val_loss: 1.6265 - val_acc: 0.4366\n",
      "Epoch 3/10\n",
      "136/140 [============================>.] - ETA: 0s - loss: 1.3155 - acc: 0.5327\n",
      "Epoch 00003: val_acc did not improve from 0.43658\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 1.3140 - acc: 0.5338 - val_loss: 1.6411 - val_acc: 0.4357\n",
      "Epoch 4/10\n",
      "136/140 [============================>.] - ETA: 0s - loss: 1.1862 - acc: 0.5809- ETA: 0s - loss: 1.1354 \n",
      "Epoch 00004: val_acc did not improve from 0.43658\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 1.1833 - acc: 0.5820 - val_loss: 1.7128 - val_acc: 0.4338\n",
      "Epoch 5/10\n",
      "136/140 [============================>.] - ETA: 0s - loss: 1.0747 - acc: 0.6132\n",
      "Epoch 00005: val_acc improved from 0.43658 to 0.44210, saving model to friends_best_model.h5\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 1.0731 - acc: 0.6137 - val_loss: 1.7979 - val_acc: 0.4421\n",
      "Epoch 6/10\n",
      "136/140 [============================>.] - ETA: 0s - loss: 0.9843 - acc: 0.6432\n",
      "Epoch 00006: val_acc did not improve from 0.44210\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.9871 - acc: 0.6416 - val_loss: 1.8463 - val_acc: 0.4338\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "history2 = model2.fit(X_train2, y_train2, batch_size=64, epochs=10, callbacks=[es, mc], validation_data=(X_dev2, y_dev2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 1ms/step - loss: 1.5844 - acc: 0.4808\n",
      "\n",
      " 테스트 정확도: 0.4808\n"
     ]
    }
   ],
   "source": [
    "loaded_model2 = load_model('friends_best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model2.evaluate(X_test2, y_test2)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    \n",
    "    #전처리(영어, 공백, ' 을 제외한 문자 제거)\n",
    "    new_sentence = re.sub(r\"[^A-Za-z ']\", \"\", new_sentence)\n",
    "    \n",
    "    # 축약문 처리\n",
    "    temp_X2 = char_change(new_sentence)\n",
    "    \n",
    "    # 반복문자 교정\n",
    "    temp_X2 = repeat_normalize(temp_X2, num_repeats=1)\n",
    "    \n",
    "    #토큰화\n",
    "    new_sentence = text_to_word_sequence(new_sentence)  \n",
    "    \n",
    "    #Stopword(불용어) 제거\n",
    "    #new_sentence = [word for word in new_sentence if not word in stopwords.words('english')]\n",
    "    new_sentence = [word for word in new_sentence if not word in new_stopwords]\n",
    "    \n",
    "    #Lemmatization\n",
    "    new_sentence = [n.lemmatize(word, 'v') for word in new_sentence] \n",
    "    \n",
    "    # Stemming_Porter\n",
    "    #new_sentence = [p.stem(word) for word in new_sentence]\n",
    "    \n",
    "    # Stemming_Lancaster\n",
    "    #new_sentence = [l.stem(word) for word in new_sentence]\n",
    "    \n",
    "    # Stemming_SnowballStemmer\n",
    "    #new_sentence = [s.stem(word) for word in new_sentence]    \n",
    "    \n",
    "    #정수 인코딩\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence])\n",
    "    \n",
    "    #패딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len)\n",
    "    \n",
    "    score = loaded_model2.predict(pad_new)\n",
    "\n",
    "    #val = np.max(score)\n",
    "    val_index = np.argmax(score)\n",
    "\n",
    "    if(val_index==0):\n",
    "        return \"anger\"\n",
    "    elif(val_index==1):\n",
    "        return \"disgust\"\n",
    "    elif(val_index==2):\n",
    "        return \"fear\"\n",
    "    elif(val_index==3):\n",
    "        return \"joy\"\n",
    "    elif(val_index==4):\n",
    "        return \"neutral\"\n",
    "    elif(val_index==5):\n",
    "        return \"non-neutral\"\n",
    "    elif(val_index==6):\n",
    "        return \"sadness\"\n",
    "    elif(val_index==7):\n",
    "        return \"surprise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-cfee1748ee8f>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_data['Predicted'][i] = Predicted\n",
      "C:\\Users\\server\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "eval_data = pd.read_csv('./[전처리]en_data.csv', engine='python', encoding='utf-8')\n",
    "eval_data['Predicted'] = 0\n",
    "\n",
    "for i in range(len(eval_data)):\n",
    "    Predicted = sentiment_predict(eval_data['utterance'][i])\n",
    "    eval_data['Predicted'][i] = Predicted\n",
    "    \n",
    "eval_data.drop(['i_dialog'], axis='columns', inplace=True)\n",
    "eval_data.drop(['i_utterance'], axis='columns', inplace=True)\n",
    "eval_data.drop(['speaker'], axis='columns', inplace=True)\n",
    "eval_data.drop(['utterance'], axis='columns', inplace=True)\n",
    "\n",
    "eval_data.to_csv(\"result_2019512014_이동환.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
